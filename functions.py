
# -----------------------------
# üîπ BM25 INDEX
# -----------------------------

from openai import OpenAI
import httpx
import json


import json
import textwrap
from typing import Dict

import os
import faiss
import pickle
import numpy as np
from typing import List, Dict, Any
from sentence_transformers import SentenceTransformer
from rank_bm25 import BM25Okapi
from dataclasses import dataclass, field




from dotenv import load_dotenv

load_dotenv()





TOP_K = int(os.getenv("TOP_K", "10"))
EMBED_MODEL = os.getenv("EMBED_MODEL", "all-MiniLM-L6-v2") # embeddings


API_KEY = os.getenv("API_KEY")
API_BASE = os.getenv("API_BASE")

HF_LLM_MODEL = os.getenv("HF_LLM_MODEL")
SYSTEM_PROMPT = os.getenv("SYSTEM_PROMPT")





@dataclass
class DocChunk:
    text: str
    meta: Dict[str, Any] = field(default_factory=dict)




class BM25Index:
    def __init__(self, docs: List[DocChunk]):
        self.corpus = [d.text for d in docs]
        tokenized = [c.split() for c in self.corpus]
        self.bm25 = BM25Okapi(tokenized)
        self.metas = [d.meta for d in docs]

    def search(self, query: str, k: int = TOP_K):
        tokens = query.split()
        scores = self.bm25.get_scores(tokens)
        top_idx = np.argsort(scores)[-k:][::-1]
        return [
            {"text": self.corpus[i], "meta": self.metas[i], "score": float(scores[i])}
            for i in top_idx
        ]


class FaissIndex:
    def __init__(self, emb_model_name=EMBED_MODEL):
        print("[faiss] chargement mod√®le embeddings...")
        # self.model = SentenceTransformer(emb_model_name)
        try : 
            self.model = SentenceTransformer('models/'+emb_model_name)
            print( "loaded locally")
        except :
            # If not found, load the model from the internet and save it locally
            self.model= SentenceTransformer("sentence-transformers/"+emb_model_name)
            print("saving")
            self.model.save('models/'+emb_model_name)  # Save locally
            print("saved")

        self.dim = self.model.get_sentence_embedding_dimension()
        self.index = faiss.IndexFlatIP(self.dim)
        self.texts: List[str] = []
        self.metas: List[Dict[str, Any]] = []
        self.embs = None

    def add(self, docs: List[DocChunk]):
        texts = [d.text for d in docs]
        print(f"[faiss] encodage de {len(texts)} chunks...")
        embs = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=True)
        faiss.normalize_L2(embs)
        self.index.add(embs)

        self.texts.extend(texts)
        self.metas.extend([d.meta for d in docs])
        self.embs = embs if self.embs is None else np.vstack([self.embs, embs])

    def search(self, query: str, k: int = TOP_K):
        q_emb = self.model.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(q_emb)
        D, I = self.index.search(q_emb, k)
        return [
            {"text": self.texts[i], "meta": self.metas[i], "score": float(s)}
            for i, s in zip(I[0], D[0]) if 0 <= i < len(self.texts)
        ]

    # ‚úÖ Save / Load pour FAISS
    def save(self, path: str):
        os.makedirs(path, exist_ok=True)
        faiss.write_index(self.index, f"{path}/index.faiss")
        with open(f"{path}/store.pkl", "wb") as f:
            pickle.dump({"texts": self.texts, "metas": self.metas, "embs": self.embs}, f)
        print(f"[faiss] index sauvegard√© dans {path}")

    def load(self, path: str):
        self.index = faiss.read_index(f"{path}/index.faiss")
        with open(f"{path}/store.pkl", "rb") as f:
            data = pickle.load(f)
        self.texts = data["texts"]
        self.metas = data["metas"]
        self.embs = data["embs"]
        print(f"[faiss] index charg√© depuis {path}")

# -----------------------------
# üîπ Hybrid search
# -----------------------------
def hybrid_search(query: str, faiss_idx: FaissIndex, bm25_idx: BM25Index, k: int = TOP_K):
    dense = faiss_idx.search(query, k=k)
    sparse = bm25_idx.search(query, k=k)
    seen = set()
    merged = []
    for r in dense + sparse:
        cid = r["meta"].get("chunk_id")
        if cid not in seen:
            merged.append(r)
            seen.add(cid)
        if len(merged) >= k:
            break
    return merged


http_client = httpx.Client(verify=False)
client = OpenAI(
    api_key=API_KEY,
    base_url=API_BASE,
    http_client=httpx.Client(timeout=httpx.Timeout(120.0))
)

def llm_generate_api(
    prompt: str,
    model_name: str = HF_LLM_MODEL,
    max_tokens: int = 256,
    temperature: float = 0.2
) -> str:

    response = client.chat.completions.create(
        model=model_name,
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ],
        max_tokens=max_tokens,
        temperature=temperature
    )

    return response.choices[0].message.content.strip()



def decide_mode_and_response(query: str, context: str) -> Dict[str, any]:
    """
    G√©n√®re le mode (SYMPT√îMES ou MALADIE) et les questions ou fiche maladie
    √† partir de la requ√™te utilisateur et du contexte.
    """
    prompt = SYSTEM_PROMPT + "\n\n" + (
        "DOCUMENTS R√âCUP√âR√âS (contexte) :\n" + context[:3000] + "\n\n"
    ) + (
        "T√¢che:\nL'utilisateur a envoy√©:\n" + query + "\n\n"
        "1) Indique en un mot le MODE choisi (SYMPT√îMES ou MALADIE).\n"
        "2) Si MODE=SYMPT√îMES : G√©n√©re 2 √† 5 questions personnalis√©es et explique bri√®vement pourquoi.\n"
        "3) Si MODE=MALADIE : Fournis une fiche concise en fran√ßais.\n\n"
        "‚ö†Ô∏è IMPORTANT: R√©pond strictement en JSON comme ceci:\n"
        '{"mode": "SYMPT√îMES", "data": {"questions": ["question1", ...]}}\n'
        "Ne mets aucun texte suppl√©mentaire."
    )

    resp = llm_generate_api(prompt, model_name=HF_LLM_MODEL, max_tokens=600, temperature=0.0)
    # Debug
    print("DEBUG LLM raw response:", resp)

    # üîπ Extraction JSON robuste
    try:
        jstart = resp.find("{")
        jend = resp.rfind("}")
        if jstart != -1 and jend != -1 and jend > jstart:
            jtxt = resp[jstart:jend + 1]
            parsed = json.loads(jtxt)
            mode = parsed.get("mode", "UNKNOWN").upper()
            data = parsed.get("data", {})
            if mode == "SYMPT√îMES" and "questions" not in data:
                data["questions"] = []
            return {"mode": mode, "data": data}
    except Exception as e:
        print("JSON parsing error:", e)

    # Fallback si parsing √©choue
    return {"mode": "UNKNOWN", "data": {"questions": [], "raw": resp}}



def synthesize_from_passages_for_disease(passage_list: List[Dict[str, Any]], disease_name: str) -> str:
    """
    G√©n√®re une fiche compl√®te pour une maladie √† partir des passages RAG,
    en utilisant le LLM via API.
    """
    # üîπ Construire le contexte √† partir des passages r√©cup√©r√©s
    context = "\n\n".join(
        [f"Source: {p['meta'].get('source','unknown')} p{p['meta'].get('page','?')}\n{p['text']}"
         for p in passage_list]
    )

    # üîπ Construire le prompt
    prompt = (
        f"Vous √™tes un m√©decin r√©dacteur. En vous basant strictement sur le contexte ci-dessous, "
        f"r√©digez une fiche compl√®te en fran√ßais pour la maladie: '{disease_name}'. "
        "La fiche doit contenir : description, sympt√¥mes typiques, facteurs de risque, "
        "examens recommand√©s, prise en charge g√©n√©rale, sp√©cialit√© √† consulter.\n\n"
        f"Contexte (maximum 4000 caract√®res) :\n{context[:4000]}\n\n"
        "Fiche maladie :"
    )

    # üîπ Appel LLM via API
    out = llm_generate_api(
        prompt,
        model_name=HF_LLM_MODEL,
        max_tokens=400,
        temperature=0.0
    )

        # Nettoyage minimal et ajout de la mention l√©gale
    out = out.rstrip() + (
        "\n\n‚ö†Ô∏è Je ne suis pas un m√©decin, cette information est √† titre informatif uniquement."
        "\n‚ÑπÔ∏è Il est recommand√© de consulter un sp√©cialiste pour un avis m√©dical pr√©cis."
    )

    # üîπ Retourne le texte complet
    return out


def run_console_agent(faiss_idx: FaissIndex, bm25_idx: BM25Index):
    print("\n=== Agentic Medical RAG FR (API) ===")
    print("Disclaimer: Outil √† vis√©e informative uniquement.")
    patient_memory = {}

    while True:
        user = input("\nPatient (ou 'exit' pour quitter) : ").strip()
        if user.lower() in ("exit", "quit"):
            break

        passages = hybrid_search(user, faiss_idx, bm25_idx, k=TOP_K)
        context_text = "\n\n".join([p["text"] for p in passages])

        dec = decide_mode_and_response(user, context_text)
        mode = (dec.get("mode") or "").upper()
        data = dec.get("data", {})

        if mode == "SYMPT√îMES":
            questions = data.get("questions", [])
            if not questions:
                print("\n[Assistant] Aucun question g√©n√©r√©e par le mod√®le.")
                continue

            answers = {}
            print("\n[Assistant] Pour mieux comprendre, j'ai quelques questions :")
            for i, q in enumerate(questions, start=1):
                if not q:
                    continue
                a = input(f"[Q{i}] {q}\n> ").strip()
                answers[f"q{i}"] = {"q": q, "a": a}
                patient_memory[q] = a

            # Refaire une recherche enrichie avec les r√©ponses
            enriched_query = user + " " + " ".join(v["a"] for v in answers.values())
            passages2 = hybrid_search(enriched_query, faiss_idx, bm25_idx, k=TOP_K)
            context2 = "\n\n".join(
                [f"Source:{p['meta']['source']} p{p['meta']['page']}\n{p['text']}" for p in passages2]
            )

            final_prompt = (
                SYSTEM_PROMPT + "\n\n"
                "Contexte m√©dical r√©cup√©r√©:\n" + context2[:4000] + "\n\n"
                f"L'utilisateur a ces sympt√¥mes: {user}\n"
                f"R√©ponses utilisateurs aux questions: {json.dumps(answers, ensure_ascii=False)}\n\n"
                "1) Propose 1-3 hypoth√®ses plausibles, en expliquant bri√®vement pour chaque pourquoi.\n"
                "2) Indique quel(s) examen(s) initial(aux) serait pertinent.\n"
                "3) Propose quelle sp√©cialit√© consulter en priorit√©.\n"
            )
            final_out = llm_generate_api(final_prompt, model_name=HF_LLM_MODEL, max_tokens=500, temperature=0.0)
            print("\n[Assistant] Synth√®se & hypoth√®ses :\n")
            print(
    textwrap.indent(
        final_out.rstrip() + (
            "\n\n‚ö†Ô∏è Je ne suis pas un m√©decin, cette information est √† titre informatif uniquement."
            "\n‚ÑπÔ∏è Il est recommand√© de consulter un sp√©cialiste pour un avis m√©dical pr√©cis."
        ),
        "  "
    )
)


        elif mode == "MALADIE":
            disease_name = user.strip()
            passages_d = hybrid_search(disease_name, faiss_idx, bm25_idx, k=TOP_K)
            fiche = synthesize_from_passages_for_disease(passages_d, disease_name)
            print("\n[Assistant] Fiche maladie :\n")
            print(textwrap.indent(fiche.strip(), "  "))

        else:
            print("\n[Assistant] Mode inconnu. Le mod√®le n'a pas pu d√©terminer SYMPT√îMES ou MALADIE.")


            #  LOAD INDEXES (fast)

# import pickle

# INDEX_DIR = ".\indexes"

# # Charger FAISS
# faiss_idx = FaissIndex(EMBED_MODEL)
# faiss_idx.load(f"{INDEX_DIR}/faiss")

# # Charger BM25

# with open(f"{INDEX_DIR}/bm25.pkl", "rb") as f:
#     bm25_idx = pickle.load(f)

# print("‚úÖ Index charg√©s")



# # RUN CONSOLE

# run_console_agent(faiss_idx, bm25_idx)

