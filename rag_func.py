from function import *



conversation_history = []  # m√©moire du chat




def get_medical_context(drug_list, df):
    context_parts = []

    for drug in drug_list:
        # üîπ match insensible √† la casse
        mask = df["canonical"].astype(str).str.lower() == drug.lower()
        rows = df[mask]

        if rows.empty:
            print(f"‚ö†Ô∏è Aucun m√©doc trouv√© dans le dataset pour : {drug}")
            continue

        row = rows.iloc[0]
        raw_card = row.get("med_card", "")

        # si pas de carte ‚Üí on peut soit sauter, soit mettre un placeholder
        if pd.isna(raw_card) or raw_card in ["", "nan", None]:
            print(f"‚ö†Ô∏è med_card vide pour : {drug}")
            continue

        try:
            card = json.loads(raw_card)
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur JSON pour med_card de {drug} : {e}")
            continue

        context_parts.append(json.dumps(card, ensure_ascii=False, indent=2))

    return "\n\n".join(context_parts)




def build_rag_prompt(question, context):
    return f"""
Tu es un expert m√©dical sp√©cialis√© dans les interactions m√©dicamenteuses.

Contexte clinique provenant des cartes m√©dicales extraites du dataset :

{context}

R√®gles :
- Utilise PRIORITAIREMENT ce contexte pour r√©pondre.
- Tu peux compl√©ter avec tes connaissances m√©dicales internes si n√©cessaire.
- R√©pond toujours de mani√®re claire, simple et exacte.
- Mentionne explicitement les interactions possibles.
- Donne √©ventuellement des recommandations pratiques.

Question :
{question}

R√©ponse :
"""



def ask_medical_question(question, drug_list, df):
    context = get_medical_context(drug_list, df)
    prompt = build_rag_prompt(question, context)

    response = client.chat.completions.create(
        model="hosted_vllm/Llama-3.1-70B-Instruct",
        messages=[
            {"role": "system", "content": "Assistant m√©dical fiable et prudent."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.2,
        max_tokens=500
    )

    return response.choices[0].message.content.strip()





def ask_about_prescription(question, final_output, df):
    # Liste des m√©dicaments reconnus apr√®s correction + matching
    drugs = [item["drug"] for item in final_output]

    print("\nüìå M√©dicaments concern√©s par la question :", drugs)

    answer = ask_medical_question(question, drugs, df)

    return answer







def ask_medical_question_conversational(question, drug_list, df):
    # 1. Construire contexte m√©dical
    context = get_medical_context(drug_list, df)

    # 2. Construire le message utilisateur pour ce tour
    user_message = f"""
Contexte des m√©dicaments :
{context}

Question de l'utilisateur :
{question}
"""

    # 3. Construire la conversation compl√®te
    messages = [{"role": "system", "content": "Assistant m√©dical expert, prudent, clair et fiable."}]

    # Ajouter historique
    for entry in conversation_history:
        messages.append({"role": entry["role"], "content": entry["content"]})

    # Ajouter le nouveau message
    messages.append({"role": "user", "content": user_message})

    # 4. Appeler le mod√®le
    response = client.chat.completions.create(
        model="hosted_vllm/Llama-3.1-70B-Instruct",
        messages=messages,
        temperature=0.25,
        max_tokens=500
    )

    answer = response.choices[0].message.content.strip()

    # 5. Ajouter ce tour dans l'historique
    conversation_history.append({"role": "user", "content": question})
    conversation_history.append({"role": "assistant", "content": answer})

    return answer





def chat_with_prescription(question, final_output, df):
    drugs = [item["drug"] for item in final_output]
    answer = ask_medical_question_conversational(question, drugs, df)
    return answer
